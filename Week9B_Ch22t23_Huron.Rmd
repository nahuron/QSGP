---
title: "Week9B_Ch22t23_Huron.Rmd"
author: "Nicholas Huron"
date: "3/15/2018"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
#required packages
library(modelr)
library(tidyverse)
options(na.action = na.warn)
```

##Chapter 23

###Section 23.2.1
  1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?
```{r simulated data}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```

Now let's fit to a linear model and plot the results!
```{r fit lm and plot}
#fit
model1 <- sim1a %>%
  lm(y~x, .) %>%
  coef(.)
#plot it
  ggplot(sim1a) +
  geom_abline(aes(intercept = model1[1], slope = model1[2])) +
  geom_point(mapping = aes(x,y))
```

We can piece this all together and run multiple instances of the simulation illustrate the point that `lm()` does not handle extreme outliers well (they are not modeled well by the linear model that is produced and fits the rest of the data well).
```{r loop to show}
#list to hold results
models <- list(NULL, NULL, NULL, NULL, NULL)

#loop to get data points and params
for(a in 1:5){
  #data gen
  dat_hold <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2))
  
  #param
  par_hold <- dat_hold %>%
  lm(y~x, .) %>%
  coef(.)
  
  #slam them into the list!
  models[[a]] <- list(dat_hold, par_hold)
  names(models)[a] <- paste0("model", a)
}
```

Now we can plot each of the *five* example datasets and models. Let's loop it!
```{r loop the plots as one}
#cb friendly palette
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

p <- ggplot()
for(b in seq_along(models)){
  p <- p+ geom_abline(aes(intercept = models[[b]][[2]][1], slope = models[[b]][[2]][2]), color = cbbPalette[b]) +
  geom_point(mapping = aes(x,y), data = models[[b]][[1]], color = cbbPalette[b])
}
p
```
 
While somewhat difficult to see each line (they are all stacked on one another), we can see some strong outliers that are not contributing much to the fit models!  
  
###Section 23.3.3
  1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?
  
We need to re-initialize `sim1a`, so we do so first:
```{r re-init sim1a}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```

With the new object, we will fit the model first:
```{r fit loess}
loess_sim1a <- loess(y~x, data = sim1a)
```

Secondly, we generate a grid and add predictions:
```{r generate loess grid}
#grid time
grid_sim1a <- sim1a %>%
  data_grid(x)

#prediction time
grid_sim1a <- grid_sim1a %>%
  add_predictions(loess_sim1a)
```

Lastly, we need to visualize the results (green) versus `geom_smooth()` (blue):
```{r visualize loess}
p <- ggplot(data = sim1a, mapping = aes(x = x)) +
  geom_point(mapping = aes(y = y))

#visualize loess version
pl <- p +  geom_line(mapping = aes(y = pred), data = grid_sim1a, colour = "green", size = 3)

#visualize geom_smooth version
pls <- pl + geom_smooth(mapping = aes(y = y), se = FALSE)

pls
```
>They lines appear to be the same, go figure (`geom_smooth()` defaults to `"loess"`, so they should be)!
  
###Section 23.4.5
  2. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction?
  
Firstly, let's explore equations for `sim3`:
```{r sim3 explore}
sim3
model_matrix(sim3, formula = y ~ x1 + x2)
model_matrix(sim3, formula = y ~ x1 * x2)
```

Now a look at `sim4`:
```{r sim4 explore}
sim4
model_matrix(sim4, formula = y ~ x1 + x2)
model_matrix(sim4, formula = y ~ x1 * x2)
```

The use of `*` to capture interactions is convenient in formulas. It can flexibly recognize the categories in the factor `sim3$x2` as well as the continuous values in `sim4$x2` and accounts for the interaction possibilities correctly! This is a great example of a tool that lets your code work for you rather than working for your code.

  4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but itâ€™s pretty subtle. Can you come up with a plot to support my claim?
  
Let's make sure we capture the same models Hadley uses:
```{r make sim4 models}
sim4
(mod1 <- lm(y ~ x1 + x2, data = sim4))
(mod2 <- lm(y ~ x1 * x2, data = sim4))
```
 
It seems useful to check and plot residuals here to compare models:
```{r compare residuals}
#get residuals
(resid_sim4 <- sim4 %>%
  gather_residuals(mod1, mod2))

#plot residuals by model
m1 <- ggplot(data = resid_sim4, mapping = aes(x1, resid)) +
  geom_point() +
  facet_wrap(~model)

m1 + geom_point(color = "green", mapping = aes(x1, mean(abs(resid))))

m2 <- ggplot(data = resid_sim4, mapping = aes(x2, resid)) +
  geom_point() +
  facet_wrap(~model)
m2 + geom_point(color = "green", mapping = aes(x2, mean(abs(resid))))

#check mean abs resid
resid_sim4 %>%
  group_by(model) %>%
  summarise(mean(abs(resid), na.rm = T))

#try to plot as density curves?
ggplot(data = resid_sim4) +
  geom_density(mapping = aes(x = abs(resid), group = model, fill = model, alpha = 0.5))
```

Just looking at mean absolute residual alone, it may be possible to see that `mod2` has slightly smaller residuals than `mod1`, indicating a slightly better fit. The density plot of absolute residuals seems to support this? That being said, trying to look at the scatterplots of residuals and `x1` and`x2` seems to not be particularly illuminating, since it is so tough to spot much of a difference...so if I am trying to support Hadley's claim, I go with the density plot and mean absolute value residuals.
  
###Section 23.5
How do you set R to give warnings if observations are dropped when fitting a model like lm()?

If you run `options(na.action = na.warn)` in the setup chunk (or prior to model fitting), you will get a warning when there are `NA`'s that are dropped from the model!