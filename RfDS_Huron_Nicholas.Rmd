---
title: "R For Data Science Working Notebook"
author: "Nicholas A. Huron"
output:
  html_notebook: default
  pdf_document: default
---
Start with some setup code!
```{r setup}
#packages required
require(tidyverse)
require(nycflights13)
#some global params
knitr::opts_chunk$set(echo = T)
```

-----

##Chapter 3

###Section 3.1

###Section 3.2
use mpg dataframe for this exercise (can use just 'mpg' if ggplot2 is loaded)
```{r}
ggplot2::mpg
```

figure out attributes of mpg
```{r}
attributes(mpg)
```

displ is the volume displacement of the engine (size) and hwy is highway mileage in mi/gal
ggplot2 code to create a scatter plot of displ vs hwy
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy))
```

#Exercises 3.2.4
  1. just the first line here creates a grey box
```{r}  
ggplot(data = mpg)
```

  2. find number of cols and rows in mpg (rows x columns returned)

11 cols and 234 rows
```{r}
dim(mpg)
```

  3. variable drv tells us the type of drive (front vs rear vs 4wd)
```{r}
?mpg
```

  4. scatterplot of hwy vs cyl
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = hwy, y = cyl))
```

  5. scatterplot of class vs drv

this plot maps two factors (categorical vars), which means any cars that contain the same factor for each will stack on a single point... loss of info/not very informative
```{R}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = class, y = drv))
```

###Section 3.3

group points with aesthetic third class for colour
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, colour = class))
```

group points with aesthetic third class for size of point
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, size = class))
```

group points with aesthetic third class for opacity of point
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))
```

group points with aesthetic third class for opacity of point
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, shape = class))
```

>shapes are limited to <7 symbols with defaults... ruh roh

###Exercises 3.3.1
  1. 
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, colour = "blue"))  #internal ) needs to close aes prior to color specification

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), colour = "blue")   #legend disappears though
```

>Plot 1: internal ) needs to close aes prior to color specification, but this drops the legend.

  2. which data are factors in mpg?

```{r}  
?mpg
```

looking at mpg in print out, it gives the classes prior to the first row (manufacturer, model, trans, drv, fl, and class are all categorical, but year technically could be considered an integer version of one too)

displ is the only "true" continuous variable in the narrow sense, but year, cyl, cty, and hwy are all numeric

  3. map continuous variable to colour, size, and shape
```{r, eval=FALSE}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = hwy, y = cty, colour = cyl, size = displ, shape = year))
```

continuous variables cannot be mapped to shape... error kicks in. others appear to work, color goes to a gradient

  4. map some variables to multiple aesthetics
  
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = hwy, y = cty, colour = cyl, size = cyl))
```

  5. stroke aesthetic? affects thickness of lines in symbols
  
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = hwy, y = cty, stroke = cyl, colour = year, shape = trans))
```

  6. map aes to something other than var name: seems to go binary

```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = hwy, y = cty, colour = displ < 5))
```

###Section 3.5
####adding facets

changed some things up to play with a bit
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = cty, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```

facets by two categories in a single grid
the formula follows the form of y ~ x
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(drv~cyl)
```

version without faceting in rows
the '.' is the key
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(.~cyl)
```

###Exercises 3.5.1
  1.
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = cty, y = hwy)) +
  facet_grid(.~displ)
```

when using a continuous var for faceting, it facets for each unique value (messy)

  2. 
```{r}  
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = drv, y = cyl)) +
  facet_grid(drv ~ cyl)
```

faceting these two categorical vars creates a grid where the values intersect .: empty boxes mean that no rows contain those two values for the vars

  3. 
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)
```

plots displ by hwy with faceting of three rows for each of the drv options (plots separately by drv type)
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(. ~ cyl)
```

now we are plotting the same data but splitting into the 4 columns by the cyl var

  4. 
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```

Faceting can help separate data that would otherwise be stacked in a smaller space that may not be shown effectively with a colour aesthetic. That being said, it separates data into different plots, which may make it more difficult to view as a whole or compare specific pairs of groups if not displayed most effectively.

With a larger dataset, the point crowding problem can become more prominent, making faceting more favorable.

  5. 
```{r}
?facet_wrap
```

nrow in this function sets the number of rows for the plot, same goes for ncol for columns
you can change the scale of dims, change the ordering, drop certain factors for plotting, placement of labels for categories, etc.
facet_grid is set with the facets argument, which dictates the number of rows and columns based on the number of unique values in the variables in the formula put in

  6.
I actually do not know the answer to this, but i assume that the x axis is easier to read with more items. This would make sense if you are looking at differences in the response variable among groups. Side by side, this is easier to do.

###Section 3.6
compare left and right plots
left
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy))
```

right
```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
```

set the mapping linetype
```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
```

map multiple aesthetics to one plot
standard line as before
```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
```

group by drivetrain as before, but with group instead of linetype
```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))
```

color the lines by group for drivetrain, do not add a legend
```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv),
              show.legend = F
              )
```

display multiple geoms in the same plotspace, oh snap!
raw points and raw line together
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
```

you can map things globably by doing so in the ggplot command rather than individual geoms
more efficient version of the above code
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth()
```

you can set the local mapping for a layer nonetheless that overrides the global one
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point(mapping = aes(color = class)) +
  geom_smooth()
```

can also do this to show subsets of data (here use just subcompact cars for the line)
we are also dropping the SE range around the line
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point(mapping = aes(color = class)) +
  geom_smooth(data = filter(mpg, class == "subcompact"), se = F)
```

###Exercises 3.6.1
  1. 
  - line chart: geom_line
  - boxplot: geom_boxplot
  - histogram: geom_histogram
  - area chart: geom_area

  2. plot of lines and points, will be all color coded by drv, no SE region in lines
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +
  geom_point() + 
  geom_smooth(se = F)
```

  3. 

show.legend=FALSE removes the legend from the display of that particular layer
it allows us to better understand the code? it also leads to cleaner bigger plots

  4. 

se is a binary T/F argument that sets the standard error around the smooth line in the geom_smooth call

  5. 
  
 I do not think so, the individual layers in the second plot  call the same global vars from the first one

first one
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth()
```

second one
```{r}
ggplot() +
  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))
```

  6. let's recreate each plot going L to R and T to B
    
a.
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth(se=F)
```

b.  
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth(se=F, mapping = aes(group = drv))
```

c.
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +
  geom_point() +
  geom_smooth(se=F, mapping = aes(group = drv))
```

d.
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point(mapping = aes(color = drv)) +
  geom_smooth(se=F)
```

e. 
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point(mapping = aes(color = drv)) +
  geom_smooth(se=F, mapping = aes(linetype = drv))
```

f.
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point(mapping = aes(fill = drv), stroke = 5, color = "white", lwd = 5,  shape = 21)
```

###Section 3.7

sparkly diamonds and barplots
```{r}
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))
```

boxplots make statistical transformations ("stats") to display data in a summarized fashion
the stat argument of any geom has a default you can check

can use stat_count() to make the same plot as above
```{r}
ggplot(data = diamonds) +
  stat_count(mapping = aes(x = cut))
```

stats have default geoms too! that is why this works
if you have the counts tabulated already, you can switch the default stat to reflect that rather than having R calculate the counts of bins
```{r}
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)

ggplot(data = demo) +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
```

change the mapping to be proportions (rel freq)
..prop.. is a variable created by the default stat for geom_bar
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1))
```

look at the transformations more closely
```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
```

###Exercises 3.7.1
  1.

default geom for stat_summary is "pointrange"

  2.

geom_bar counts the instances of each bin across the x axis and geom_col uses values of the data as they are (identity in an above example)

  3. 

  - geom_bar geom_col stat_count
  - geom_bin2d stat_bin_2d 
  - geom_boxplot stat_boxplot 
  - geom_contour stat_contour 
  - geom_count stat_sum 
  - geom_density_2d stat_density_2d
  - geom_density stat_density 
  - geom_hex stat_bin_hex 
  - geom_freqpoly geom_histogram stat_bin 
  - geom_qq stat_qq 
  - geom_quantile stat_quantile 
  - geom_smooth stat_smooth 
  - geom_violin stat_ydensity 
  - geom_sf stat_sf

most of these obvious pairs have the same post geom_ or post stat_ nomenclature! This makes it easy to call the corresponding geom/stat function in a pair

  4.

stat_smooth computes: a predicted value of y, a lower and upper pointwise confidence interval around the mean, and standard error
The function is controlled by the following arguments found in the documentation:

`stat_smooth(mapping = NULL, data = NULL, geom = "smooth", position = "identity", ..., method = "auto", formula = y ~ x, se = TRUE, n = 80, span = 0.75, fullrange = FALSE, level = 0.95, method.args = list(), na.rm = FALSE, show.legend = NA, inherit.aes = TRUE)`

  5. 
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = ..prop..))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..))
```

these plots show the proportions as all 1 (and 1 for each color option in the second case)
they appear to be unable to understand the call for cumulative proportion...

###Section 3.8

color outlines by cut with colour argument
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))
```

or the whole bar with fill
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))
```

can make stacked bars for each x category according to another var with fill command by a discrete value
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
```

this uses position argument to an auto setting of "stack"
but we have some other options: identity, dodge, and fill (also jitter too, but it is not helpful for bar plots)
identity places portions of the bar by their individual totals within the same bar (smaller stuff to the bottom), makes it difficult to see without messing with transparency with alpha or fill=NA
```{r}
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")
```

position="fill" makes the bars all proportions and stacks them (proportion of total for that x category)
so even though ideal is larger in the diamonds dataset, both it and fair are set to the same max of 1.0 total proportion
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
```

position="dodge" puts values next to each other, so this makes a bar chart of bar charts for these data
this is purrrttyyyy
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
```
so overplotting... rounding of values when points are close (think xy scatter plots), makes it seem like there are fewer points
so set position="jitter" to get around this
adds random noise to each point to make visualization easier
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
```

geom_jitter is a shortcut to this argument, it functions just like geom_point(position="jitter")

###Exercises 3.8.1
  1. 
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_point()
```

suspect that points are overplotted, try with geom_jitter instead
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_jitter()
```

yup.

  2. 
```{r}
?geom_jitter
```

width and height control the possible range of jittering that occurs in those particular dims

  3. 
```{r}  
?geom_jitter
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_jitter()
#vs
?geom_count
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_count()
```

so geom_count works nicely when you have numerous instances of the same value. instead of jittering the points to make it seem like they are not identical (think discrete vars this is problematic), it increases point size for n same values

pretty cool alt to geom_jitter

  4.What’s the default position adjustment for geom_boxplot()? Create a visualisation of the mpg dataset that demonstrates it.

default aggregated data
```{r}
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = class, y = cty))
```

vs.

data partitioned by drv
```{r}
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = class, y = cty, fill = drv))
```

the default position for geom_boxplot is dodge! sweet
This is also in the documentation for geom_boxplot()

###Section 3.9

so coordinate systems...
coord_flip flips the  x and y axes
helpful for swapping boxplots, bar plots, etc.
original horizontal config
```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
```

now with coord_flip
```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
```

coord_quickmap() helps get maps for spatial data happy
data to play with
```{r}
nz <- map_data("nz")
```

without coord_quickmap: yucky distortion due to trying to stretch to fit plot space
```{r}
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")
```

now try this: scales equal-ish and cleaner?
```{r}
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
```

and of course coord_polar works for polar coordinates: have not used those since calc or precalc
example with diamonds
create object for calling plot!
```{r}
bar <- ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut), show.legend = FALSE,width = 1) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)
```

flip it, cuz we can
```{r}
bar + coord_flip()
```

use polar coords: looks pretty cool actually
```{r}
bar + coord_polar()
```

###Exercises 3.9.1

  1. 

looking at the coord_polar documentation, setting the x in the bar plot to be a factor grouping of 1 gives you a single stacked bar that can be easily converted to a pie chart for a single var
```{r}
bar <- ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = factor(1), fill = cut), show.legend = FALSE,width = 1, position="fill") + 
  theme(aspect.ratio = 1)
#setting theta gives the angle mapping such that you cut the pie up rather than getting a bullseye cutting up
bar + coord_polar(theta="y")
```

  2.
  
labs() configures labels for the plotting space, you can add custom ones or edit existing ones

  3. coord_quickmap vs coord_map

quickmap is a much faster approximation of the map version that preserves straight lines, while the full version uses projections to accurately depict earth's curvature in the 2d plane with lat/long

  4. 
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

they are largely linear and positively correlated, but not with a slope of 1 (hwy is generally higher than city for all data points)
coord_fixed is important, because it makes it easy to identify what y=x is (the abline command creates this line on the plot) and ensure that the scale on both axes is consistent

###Section 3.10

general form of plotting template
```{r, eval = FALSE, warning=FALSE, error=FALSE}
ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
    mapping = aes(<MAPPINGS>),
    stat = <STAT>, 
    position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>
```
-----

##Chapter 4



-----

##Chapter 6

###Section 6.1
So, if you were not using the script editor before (God bless your soul if that applies to you), you should probably do so from now on.

Command + Enter is a great shortcut to run the code your cursor is on presently. I use it 80% of the time, every time.

You can practice running a script by repeatedly pressing this shortcut through the code, it will advance through lines/commands.

```{r example for run shortcut}
#some filter function from the book
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

#group and summarise
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(mean = mean(dep_delay))
```
>You get: a table of summarise results!


If you want the whole code from the script editor to run, command + shift + s is what you are looking for.


>*Worthwhile etiquette note*: if you intend to share code with others, avoid setting global params for them, such as setwd() or install.packages() as a default. Obviously, there may be some exceptions to this, but it is worth mentioning.

###Section 6.2
Some notes on the Rstudio interface for diagnosing problems: Red squiggly lines and an "x" button will show up when there is a syntax error, such as unpaired parentheses. You can use the tooltip over the button to get more info regarding the problem.

###Section 6.3
Now we get to "practice" some of the tips and stuff. Woo.

1. Go to the RStudio Tips [twitter account](https://twitter.com/rstudiotips) and find one tip that looks interesting. Practice using it!

The tip that interests me the most is [this](https://twitter.com/rstudiotips/status/865613484121137152) Sublime-style multiple cursors trick! You can easily add a common suffix to the matching text...

So I have this little code chunk from another script that I am using regex for substituting values. It won't run, but I can easily change it from this:
```{r example before, eval=F}
#merge the subspecies
dat.tt$species <- gsub("Leiocephalus lunatus lewisi", "Leiocephalus lunatus", dat.tt$species)
dat.tt$species <- gsub("Leiocephalus lunatus melaenacelis", "Leiocephalus lunatus", dat.tt$species)
dat.tt$species <- gsub("Leiocephalus psammodromus hyphantus", "Leiocephalus psammodromus", dat.tt$species)
```
to this (as long as I realize it will change all of them if I do not stay in the selected text):
```{r example after, eval=F}
#merge the subspecies
dat.tt$species <- gsub("L. lunatus lewisi", "L. lunatus", dat.tt$species)
dat.tt$species <- gsub("L. lunatus melaenacelis", "L. lunatus", dat.tt$species)
dat.tt$species <- gsub("L. psammodromus hyphantus", "L. psammodromus", dat.tt$species)
```

2. What other common mistakes will RStudio diagnostics report? Read [this](https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics) to find out.

  - can confirm that arguments in functions are included and have appripriate syntax
  - can check that variables are appropriate (e.g., no definition in scope, defined but not used)
  - style diagnositics, such as whitespace
  - can even do diagnostics in other code languages
  - some of these same diagnostics can be done for entire projects too

------

##Chapter 7

###Section 7.1
####Exploratory Data Analysis
  1. come up with questions about dataset
  2. seek answers via visualization, transformation, and modeling the data
  3. refine question based on step 2 and generate new questions
  
```{r add some required packages}
library(tidyverse)
```

###Section 7.2
When doing EDA, there are no inherently "wrong" questions, but some particularly helpful ones follow along these lines:

  1. What type of variation occurs within variables?
  2. What type of covariation occurs between variables?

Data are **tidy** if they have one value per cell, one variable per column, and one observation per row for a particular set of tabulated data.

###Section 7.3
**Variation** is the phenomenon that variable values change from measurement to measurement. 

It is worth noting the distinction between **continuous** and **categorical** variables. The former are numeric and can "take any of an infinite set of ordered values." Conversely, the latter are easily separated into discrete sets, typically as factors in R.

Here is an example of **categorical** variable visualization:
```{r barplot}
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))
```
The `dplyr::count()` function can get the counts by category as well:
```{r get counts}
diamonds %>%
  count(cut)
```
\   
Now on to a **continuous** variable example with a histogram:

```{r histogram}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.25)
```

Can compute the histogram values manually with `dplyr::count()` and `ggplot2::cut_width()`:

```{r example with count and cut_width}
diamonds %>%
  count(cut_width(carat, 0.25)) #note: i used binwidth 0.25, not 0.50
```

You should always experiment with binwidth for histograms!

Here is an example with smaller bins for a subset of the diamonds data:

```{r smaller binwidth histogram}
smaller <- diamonds %>% 
  filter(carat < 3)
  
ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.1)
```

This shows how much can be hidden in a single bin...

When you want to do multiple histograms, consider `geom_freqpoly()`. It calculates the same but uses a line instead.

```{r freqplot}
ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +
  geom_freqpoly(binwidth = 0.1)
```

Outliers can be tricky to spot. Take this example:
```{r outlier y diamonds}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)
```
The only odd part here is the range of y, relative to the visible barplot. What is the deal? To spot the odd outliers, we can use `coord_cartesian()`:

```{r look at outliers}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 50))
```
Now we can see the outliers better! We can extract them in the table too. It turns out that these are incorrect data transcription!
```{r look at outliers table}
unusual <- diamonds %>% 
  filter(y < 3 | y > 20) %>% 
  select(price, x, y, z) %>%
  arrange(y)
unusual
```

####Exercises 7.3.4
  1. Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? How can you figure out which ones are length, width, and depth?
  
```{r look at diamond dims}
  #first try to redo table
  dia_x <- diamonds %>%
    transmute(meas = x, shape = "x")
  dia_y <- diamonds %>%
    transmute(meas = y, shape = "y")
  dia_z <- diamonds %>%
    transmute(meas = z, shape = "z")
  #combine
  dia_xyz <- rbind(dia_x,dia_y, dia_z)
  
#now plot all of them together
  ggplot(data = dia_xyz, mapping = aes(x = meas, fill = shape)) +
    geom_density(alpha = 0.8)
  
#or separately
  ggplot(data = dia_x, mapping = aes(x = meas)) +
    geom_density(alpha = 0.5, fill = "red") +
    xlim(c(0,20))
  ggplot(data = dia_y, mapping = aes(x = meas)) +
    geom_density(alpha = 0.5, fill = "green")+
    xlim(c(0,20))
  ggplot(data = dia_z, mapping = aes(x = meas)) +
    geom_density(alpha = 0.5, fill = "blue")+
    xlim(c(0,20))
```
So I plotted the density plots of all three together, and there are definitely clear outliers based on the x axis (the values for each dimension). It is clear that the **Y and Z** variables contain some extreme outliers (2 and 1 values, respectively). **X** does not have this same problem! When you set clear limits for the measurements, such that the values range from 0--20, it looks like **X** and **Y** are closer to each other, while **Z** is slightly smaller sometimes.
```{r with limits}
 ggplot(data = dia_xyz, mapping = aes(x = meas, fill = shape)) +
  geom_density(alpha = 0.5) + 
  xlim(c(0,20))
```
It sure seems like **X and Y** are the two dimensions at the face of the diamonds and Z is the depth! The first two appear to have the same dimensions, since they turned the fill a brown green color.
  
  
  2. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)
  
  So let's plot price with `geom_histogram()`. I started with the standard deviation (SD) as a binwidth (use `sd()`) and preceeded to split bins into smaller groups by dividing the SD by 10^X^.
  
  FYI: the SD is `sd(diamonds$price)` and is almost 4000 units.
  
```{r prices bar graph}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram(binwidth = (sd(diamonds$price)/100))
```
When we divided by 10^2^, we see a strange break somewhere at 0--2500, likely around 1500. Let's replot it on a narrower x limit and use `count()` and `cut_width()` to look more into it, but we need to break the groups up more evenly:
```{r look for gap}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram(binwidth = (sd(diamonds$price)/100)) +
  xlim(c(0, 2500))

diamonds %>%
  count(cut_width(price, 100))
```

  >**We can see the gap is right around 1500!**
  
  3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?

Let's figure out 0.99 carats:
```{r 0.99}
diamonds %>%
  filter(carat == 0.99) %>%
  summarise(n())
```
There are *23* diamonds at 0.99 carats!

Let's do the same for 1.00 carats:
```{r 1.00}
diamonds %>%
  filter(carat == 1.00) %>%
  summarise(n())
```
There are *1558* 1.00 carat diamonds! Clearly, the value of being 1 carat is a big deal in the diamond industry!!!

  4. Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?
```{r}
  ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram(binwidth = (sd(diamonds$price)/100)) +
  xlim(c(0, 2500))
```
```{r}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram(binwidth = (sd(diamonds$price)/100)) +
  coord_cartesian(c(0, 2500))
```

The biggest difference, as mentioned in the text, is that xlim will truncate the graph, leading to loss of bins that contain data (if outside the expected range), whereas coord_cartesian will graphically zoom in but retain the actual dataset.

Let's leave binwidth unset for each:
```{r}
  ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram() +
  xlim(c(0, 2500))
```
```{r}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram() +
  coord_cartesian(c(0, 2500))
```

They both default to 30 bins! xlim sets the 30 bins based on the window being viewed, but the coord_cartesian uses 30 bins for the whole dataset, even when not visible in the window.

Now how about half a bar:
```{r}
  ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram(bins=1) +
  xlim(c(250, 350))
```

```{r}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram(bins = 1) +
  coord_cartesian(c(250, 350))
```
Here, xlim sets the bins for the window, producing a grey box of the area of the plot, whereas the coord_cartesian retains the original edges instead.

###Section 7.4
####Data Abnormalities and NA's
Dealing with missing values or 'wonky' values, you can:
  1. filter the data to avoid the rows with values that are problematic
```{r}
dim(diamonds)  
dim(diamonds2 <- diamonds %>% 
  filter(between(y, 3, 20)))
```
  2. Or, you can replace odd values with NA. `mutate()` and `ifelse()` can get this done!
```{r}
(diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y)))
```

Ggplot2 will warn that values are missing from a plot, which is good data science etiquette. `na.rm=TRUE` will suppress the warning by dropping such values. Sometimes you want to separate out the NA's to compare:
```{r NA compare}
nycflights13::flights %>% 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)
```
####Exercises 7.4.1
  1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference?
  
Histogram:

```{r na in hist}
(diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y)))

ggplot(diamonds2) +
  geom_histogram(mapping = aes(x = y))
```
Histograms remove Na values by default!

Bar chart:
```{r na in bar}
(diamonds2 <- diamonds %>% 
  mutate(color = replace(x = diamonds$color,diamonds$color == 'E', NA)))
ggplot(diamonds2) +
  geom_bar(mapping = aes(x = color))
```
NA's get their own category in a bar chart!

  2. What does na.rm = TRUE do in mean() and sum()?
This command will remove the NA values in a dataset and then calculate the corresponding metric!

###Section 7.5
####Covariation!
**Covariation** describes the tendency of 2+ variables to vary together in a way that suggests as relationship.

Depending on the type of variables involved, visualisation best practices differ:
  1. categorical and continuous variable pair
  If counts differ significantly, density plots are the way to go!
  
```{r density plot}
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

  You can also use a boxplot!

>NOTE: points that are >1.5 times the interquartile range limits (IQR, or the 25th--75th percentile) are shown. The *whisker* goes to the farthest non-outlier point.

```{r boxplot}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()
```
>If you want to re-order a categorical variable, use `reorder()` to do so!

Compare the following plots: 
```{r unordered vs ordered}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) +
  geom_boxplot()
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
```

Just like with barplots, `coord_flip()` helps with long variable names.
```{r coordflip + ordered}
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip()
```

####Exercises 7.5.1
  1. Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights.
```{r improved flights}
flights2 <- flights %>% 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
    )

  ggplot(data = flights2, mapping = aes(x = sched_dep_time, y = ..density..)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)

  ggplot(data = flights2) +
    geom_boxplot(mapping = aes(x = cancelled, y = sched_dep_time, fill = cancelled), show.legend = F) +
    coord_flip()
```
With both of these, we can get some better idea of how the two data partitions compare!

  2. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?
  
>It probably has to do with how big the rock is... even though this whole section is about **categorical** variables.

```{r diamonds}
diamonds %>%
  arrange(desc(price)) %>%
ggplot(mapping = aes(x = carat, y = price)) +
  geom_point() +
  geom_smooth()
```

>*Yup, that is definitely the case.*
 
  3. Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()?
  
```{r install ggstance, results = 'hide'}
install.packages("ggstance", dependencies = T, verbose = F)
library(ggstance)
```
```{r horizontal plot vs coordflip}
ggplot(data = mpg) +
  geom_boxploth(mapping = aes(y = class, x = hwy))

ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = class, y = hwy)) +
  coord_flip()
```
> **The key difference is that when using `ggstance::geom_boxploth`, you have to set `x = ` and `y = ` as you would after rotation, whereas with `coord_flip()`, you treat it as before**

  4. One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots?
```{r install lvplot, results = 'hide'}
install.packages("lvplot", dependencies = T, verbose = F)
library(lvplot)
```
```{r geom_lv}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_lv(outlier.colour = "blue")
```
So this type of plot uses k letter grupings to build out density pixels. Outliers are still separate blocks, but now the shape of the distribution is far clearer for each (e.g., ideal cut diamonds are skewed towards lower prices). Most of the fair cut diamonds are clustered towards the cheaper end, like other groups. However, there are numerous outliers for this group.

  5. Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method?

So let's compare the different style plots in the order of this question:
```{r compare plot styles}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_violin()
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) +
  geom_histogram() +
  facet_grid(facets = cut ~ .)
ggplot(data = diamonds, mapping = aes(x = price, y= ..density.., color = cut)) +
  geom_freqpoly()
```
Some notes on each type:
  - violin: This method shows distribution shape nicely, but it fails to highlight outliers well and narrow tails may misinform interpretation.
  - facetted histograms: As with all histograms, binning can make the data display a certain pattern if one is not careful. That being said, it is easier to see narrow distrubtion tail shape.
  - freqpoly: The color scheme shows the distributions on the same space, making comparisons easier in principle. However, density plots are required to make it a tad easier to interpret.
  
  6. If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.

Install the package to make sure we can look at the documentation:
```{r install ggbeeswarm, results = 'hide'}
install.packages("ggbeeswarm", dependencies = T, verbose = F)
library(ggbeeswarm)
```
Methods listed:
  - `geom_beeswarm`-- Points are jittered so that there are no overlapping points
  - `geom_quasirandom`-- Points are jittered in a beeswarm but overlapping is avoided in one dimension, creating lines of dots in some cases!

  2. Two categorical variables
  
Use `geom_count`
```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))
```

You can do it manually with `count` and then visualize with a heatmap style:
```{r}
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))
```

>If variables are unordered, you can use `seriation` package to reorder rows and columns!

####Exercises 7.5.2
  1. How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut?

You could do the proportion of totals! To do this, you need to remember that the counts are called "n".
```{r proportions by grouping}
#get the data
(colorcut <- diamonds %>%
  count(color, cut) %>%
  group_by(color) %>%
  mutate(proportion = n / sum(n)))
#plot the data
ggplot(data = colorcut, mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = proportion))
```

>Boom! we can see that ideal cut G color is the highest proportion here.

  2. Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?

```{r}
flights %>%
  group_by(dest, month, year) %>%
  summarise(mean_dep_delay = mean(dep_delay)) %>%
  ggplot(mapping = aes(x = month, y = dest)) +
  geom_tile(mapping = aes(fill = mean_dep_delay))
```

Wow there are a lot of NA values messing with this (some dest have NA for a particular month)... so why not remove them?

```{r}
flights %>%
  group_by(dest, month, year) %>%
  summarise(mean_dep_delay = mean(dep_delay, na.rm = T)) %>%
  filter(!is.na(mean_dep_delay)) %>%
  ggplot(mapping = aes(x = month, y = dest)) +
  geom_tile(mapping = aes(fill = mean_dep_delay))
```

>That looks better!

  3. Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above?
  
This is what option 2 looks like:
```{r}
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(y = color, x = cut)) +
    geom_tile(mapping = aes(fill = n))
```

>Cut has a logical order for categories. What the heck does D vs J color mean to the layperson?! If we have the X axis in a continuum, that seems better! So stick to the original.

  3. Two continuous variables

Scatterplots!
```{r}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = carat, y = price))
```

For big datasets, it can get hard to see... you can set transparency with `alpha` to fix this:

```{r}
ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)
```

Another solution is to use 2D binning!
```{r}
ggplot(data = smaller) +
  geom_bin2d(mapping = aes(x = carat, y = price))

install.packages("hexbin")
library(hexbin)
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))
```

Or... we can group/bin our continuous vars to look like categorical ones:

```{r}
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))
```

We can also set `varwidth = TRUE` to show sample size for each group.
```{r}
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)), varwidth = TRUE)
```

We can also set the bin values to include similar sample sizes:
```{r}
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))
```

####Exercises 7.5.3
  1. Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price?
  
I donnu, let's try it?
```{r}
ggplot(data = diamonds) + 
  geom_freqpoly(mapping = aes(x = price, y= ..density.., color = cut_width(carat, 0.5)), binwidth = 500)
ggplot(data = diamonds) + 
  geom_freqpoly(mapping = aes(x = price, y= ..density.., color = cut_number(carat, 10)), binwidth = 500)
```

So it is clear to me that when using either, you need to be cognizant of the bin sizes and number of unique groups you will get when separating out the data. Sometimes it will look janky, because you paritioned things too much or too little. Again, this can confuse patterns in the data visualisation, making interpretation difficult.


  2. Visualise the distribution of carat, partitioned by price.
  
```{r}
library(hexbin)
ggplot(data = diamonds) +
  geom_hex(mapping = aes(y = carat, x = price))
ggplot(data = diamonds, mapping = aes(x = price, y = carat)) + 
  geom_boxplot(mapping = aes(group = cut_width(price, 1000)), varwidth = TRUE)
```

>We have two types of plots:
  - the hexplot, which bins in both directions.
  - the boxplots, which bins the prices by 1000 increments with scaling of boxplots to the sample size!
  
  3. How does the price distribution of very large diamonds compare to small diamonds. Is it as you expect, or does it surprise you?

So let's get the larger set:
```{r larger diamonds}
(larger <- diamonds %>% 
  filter(carat >= 3))

ggplot(data = larger, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.25)), varwidth = TRUE) +
  ggtitle(label = "Larger Diamonds")

ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.25)), varwidth = TRUE) +
  ggtitle(label = "Smaller Diamonds")
```

Looking at the plots side by side, it is surprising! I expected the same trend of increasing median value for the size bins, but that is not the case. Median value for >3.5 carats dips before recovering slightly. Notably, this is a smaller dataset with only n=40.

  4. Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price.
  
```{r}
library(hexbin)
diamonds %>%
  group_by(cut) %>%
ggplot(mapping = aes(x = carat, y = price)) +
  geom_hex(mapping = aes(fill = cut), alpha = 0.5)

library(viridis)
diamonds %>%
  group_by(cut) %>%
ggplot() +
  geom_hex(mapping = aes(y = carat, x = price)) +
  facet_grid(facets = cut ~ ., as.table = T) +
  scale_fill_viridis(end = 0.85, direction = -1) +
  xlim(c(min(diamonds$price),max(diamonds$price))) +
  ylim(c(min(diamonds$carat), max(diamonds$carat)))
```
  5. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.

```{r data}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))
```

Why is a scatterplot a better display than a binned plot for this case?

> A binned plot would likely hide some of these outliers by grouping them more tightly with the rest of the distribution. For boxplots, they would need to fall well outside the data, whereas binning would lead to them appearing less severe of outliers.

###Section 7.6
####Patterns and Models
Remove the carats and price relationship with a linear model and plot residuals:
```{r}
library(modelr)

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

ggplot(data = diamonds2) + 
  geom_point(mapping = aes(x = carat, y = resid))

ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

Now we can see the cut relationship that we expected!

###Section 7.7
####Ggplot2 Calls
We are now going to drop some of the supporting argument calls, like `data`, `mapping`, `x`, `y`, etc. for `ggplot()` and `aes()`.

We will also pipe in to plot data (use `%>%` prior to `ggplot()`).

-----

##Chapter 8
Reference this to use rstudio projects!

###Section 8.1
Command + Shift + F10 restarts R

-----

##Chapter 9
Tidying Data Intro

-----

##Chapter 10

###Section 10.1
####Introduction to Tibbles
**Tibbles** are just a newer type of data frame that does not break the R base code! We use the `tidyverse` package to access the `tibble` package. I have already loaded the package in the setup chunk, but the chunk below would work as well.

```{r load tidyverse, eval = F}
library(tidyverse)
```

###Section 10.2
####Creating Tibbles
You can convert regular dataframes to `tibble`s by using `as_tibble()`.
```{r}
as_tibble(iris)
```

You can also create a new tibble from vector data with `tibble()`. Tibbles do a bunch of nice things that data.frames do not:
  - does not create row names
  - can use typically illegal (non-syntactic) col names
  - doesn't change strings to factors sometimes
  - will not change variable names
  
```{r}
tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)
```

To use the typically illegal names, you have to use backticks (\`). This convention will work in other `tidyverse` packages.

```{r}
tb <- tibble(
  `:)` = "smile", 
  ` ` = "space",
  `2000` = "number"
)
tb
```

You can also use transposable tibbles with `tribble()`. To do so, you have to enter column headings as formulas and elements are separated by commas.
```{r}
tribble(
  ~x, ~y, ~z,
  #--|--|---- this commenting shows where the header is
  "a", 2, 3.6,
  "b", 1, 8.5
)
```

###Section 10.3
####Tibbles vs data.frame

Two main differences:
  1. printing
  2. subsetting

Tibbles only print the first 10 rows and cols in the width of the window. They also include the class of each col.

```{r}
  tibble(
  a = lubridate::now() + runif(1e3) * 86400,
  b = lubridate::today() + runif(1e3) * 30,
  c = 1:1e3,
  d = runif(1e3),
  e = sample(letters, 1e3, replace = TRUE)
)
```

You can manually change this to view all of a tibble though! Using `print()`, you can set the number of rows with `n=` and number of cols with `width=` (note: `width = Inf` gets all cols printed).

```{r}
nycflights13::flights %>% 
  print(n = 10, width = Inf)
```

You can change the default behavior of tibbles with: `options(tibble.print_max = n, tibble.print_min = m)`. Replace n and m with the values you want. To print all rows, use `options(dplyr.print_min = Inf)`.

To subset, you can use `$` or `[[]]`. The former uses name only, while the latter can use col number and name.

```{r}
df <- tibble(
  x = runif(5),
  y = rnorm(5)
)

# Extract by name
df$x
df[["x"]]

# Extract by position
df[[1]]
```

  >To use in a pipe, you have to use a special character placeholder for the object to subset: `.$`

```{r}
df %>% .$x
df %>% .[["x"]]
```

  >Some older functions do not work with tibbles. use `as.data.frame()` to make it a dataframe again.
  
The ambiguity of `[]` return object is what gets problematic! Tibbles will always return tibbles!

####Exercises 10.5
  1. How can you tell if an object is a tibble? (Hint: try printing mtcars, which is a regular data frame).
  
Tibbles will always say they are a tibble and specify the dimensions as a comment at the top of the printed output! *Regular data frames do not do this.*

  2. Compare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data frame behaviours cause you frustration?
  
```{r tibble vs data.frame, eval=F}
  df <- data.frame(abc = 1, xyz = "a")
  df$x
  df[, "xyz"]
  df[, c("abc", "xyz")]
```

Try this with the default dataframe:
```{r tibble vs data.frame: regular}
  df <- data.frame(abc = 1, xyz = "a")
  names(df)
  df$x
  df[, "xyz"]
  df[, c("abc", "xyz")]
```

Now try with a tibble:
```{r tibble vs data.frame: tibble}
  df <- as_tibble(data.frame(abc = 1, xyz = "a"))
  df$x
  df[, "xyz"]
  df[, c("abc", "xyz")]
```

  >We can see that the default data frame method manages to return a value for subsetting by `x`, which is not a col name (`xyz` is). The tibble method does not do this (kicks back an error), which makes it more robust, even though they perform the same for simple cases of subsetting!
  
  3. If you have the name of a variable stored in an object, e.g. var <- "mpg", how can you extract the reference variable from a tibble?
  
  If you do not know that the variable is named "mpg", you can use the command `names()` to determine that `mpg` is a part of the `df` tibble. From there, you can use either of the subsetting methods to call the subset. You can call the subset on it's own with the `$` or `[[]]` notation or pipe in with a `.` in front of the `$` or `[[]]`, which you use normally as below.
```{r}
df <- as_tibble(data.frame(abc = 1, xyz = "a", mpg = seq(0,50,5)))
#get names
names(df)
#reference mpg
df$mpg
df[["mpg"]]
#filter to just have mpg
df %>%
  .[["mpg"]]
df %>%
  .$mpg
```

  4. Practice referring to non-syntactic names in the following data frame by:
  
```{r object annoying}
annoying <- tibble(
  `1` = 1:10,
  `2` = `1` * 2 + rnorm(length(`1`))
)
```
   
  - Extracting the variable called 1.

```{r pull just 1}
  annoying$`1`
  annoying[["1"]] #not using the same character to pull
```
 
  - Plotting a scatterplot of 1 vs 2.
  
```{r plot cols of annoying}
ggplot(data = annoying) +
  geom_point(mapping = aes(x = `1`, y = `2`))
```
  
  - Creating a new column called 3 which is 2 divided by 1.
  
```{r create a 3rd col}
  annoying <- annoying %>%
    mutate(`3` = `2`/`1`)
```
  
  - Renaming the columns to one, two and three.

```{r rename cols of annoying}
#change names to character version
  names(annoying) <- c("one", "two", "three")
  annoying
#we can change the names back with `rename()`
  (annoying %>%
    rename(., `1` = one, `2` = two, `3` = three))
```

  > **`rename()`** works with where you call the object in a pipe with a `.` and then list out the new name equals the old name (e.g., ``1``=one).
  
  5. What does tibble::enframe() do? When might you use it?
  
`tibble::enframe()` converts a vector to a tibble dataframe, which includes a column of the names of the vector elements (defaults to the values if no names exist) and a column of the values. The arguments `name` and `value` can be used to set names for the new cols. You may use it when you want to turn a vector into a tibble... I am not certain *exactly* when you would want to beyond that, but I am sure there are even more specific conditions.

  6. What option controls how many additional column names are printed at the footer of a tibble?
  
`options(tibble.max_extra_cols = n)` will change the number (n) of extra variable (col) names that are printed in the footer of tibbles! You can use `tibble-options` to learn more in the help search.

-----

##Chapter 11
 
###Section 11.1
####Data Import Introduction

This chapter will focus on using *your own* data, rather than pre-loaded data in R packages. We will need the `readr` package, which belongs to the `tidyverse` package group.

###Section 11.2
####Getting Started

`readr` has a bunch of functions that turn flat data into data frames:
  - `read_csv()` reads comma separated files
  - `read_csv2()` reads semicolon separated files
  - `read_tsv()` reads tab separated files
  - `read_delim()` reads files with any sort of delimiter (you just need to specify the delimiter in the `delim =` argument)
  - `read_fwf()` reads fixed width files (set by `fwf_widths()` or `fwf_positions()`)
  - `read_table()` reads a typical version of fixed width files by recognizing white space separation
  - `read_log()` reads apache style log files
  
Here is an example with `read_csv()`:
```{r reading csv, eval=F}
heights <- read_csv("data/heights.csv")
```

Note that the cols will be printed with their name and class.

You can also do inline CSV files:
```{r}
read_csv("a,b,c
         1,2,3
         4,5,6")
```

The first line consists of the colnames separated by commas. There are some cases where you may want to modify this behavior:
  1. Metadata at the top of a file. Use `skip = n` to avoid reading those lines or `comment = "#"` to drop all lines that start with `#` (or another character of your choosing).
```{r}
read_csv("The first line of metadata
  The second line of metadata
  x,y,z
  1,2,3", skip = 2)
read_csv("# A comment I want to skip
  x,y,z
  1,2,3", comment = "#")
```

  2. A file without colnames. Set `col_names = FALSE` to not read in the first row of values and label each col with X1, X2, etc.
```{r}
  read_csv("1,2,3\n4,5,6", col_names = FALSE)
```

  >NOTE: `\n` will create a new line in a string.
  
You can also set colnames with the `colnames =` parameter too with a concatenated list of strings. Of note, you can set what values should be considered `NA` with `na=` (e.g., `na=.` will set periods to `<NA>`).
  
###Exercises 11.2.2
  1. What function would you use to read a file where fields were separated with “|”?
  
You would use `read_delim()` and would set `delim = "|"` as a parameter.

  2. Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?
  
They contain the following same arguments:

  - col_names
  - col_types
  - locale: controls default time zone, decimal, dates, etc.
  - na
  - quoted_na: treat missing values inside quotes as NA?
  - quote
  - trim_ws: should leading and trailing whitespace be trimmed
  - n_max: max number of records to read
  - guess_max: max number of records to use to guess col type
  - progress: show progress bar?
  
  3. What are the most important arguments to read_fwf()?
  
Aside from specifying the `file`, setting the `col_positions` or `col_types` are probably the most important. Specifically, `col_positions` links to `fwf_widths` or `fwf_positions`, which help dictate how to read and parse (separate) the file to be a dataframe.

  4. Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like " or '. By convention, read_csv() assumes that the quoting character will be ", and if you want to change it you’ll need to use read_delim() instead. What arguments do you need to specify to read the following text into a data frame?

  `"x,y\n1,'a,b'"`
  
This is what it looks like as a `read_csv` without any edits:
```{r try with read_csv}
read_csv("x,y\n1,'a,b'")
```

Now let's use `read_delim` to bypass this problem:
```{r try with read_delim}
read_delim(file = "x,y\n1,'a,b'", quote = "\'", delim = ",")
```

>We need to use the `quote="\'"` parameter setting to bypass the single quotes problem! We also need to make sure we specify `file =` and `delim =` as usual too.

  5. Identify what is wrong with each of the following inline CSV files. What happens when you run the code?
  
```{r}
read_csv("a,b\n1,2,3\n4,5,6")
```

>When you run this code, you lose your third elements, because you have not specified a third col or set up elements/observations in a 3x2 format.

```{r}
read_csv("a,b,c\n1,2\n1,2,3,4")
```

>This command has set a new line with `\n` one number too early or lacks even observations relative to the names of columns. Because of this, `NA` is entered for the third element of the first row and the fourth observation is dropped from the table.

```{r}
read_csv("a,b\n\"1")
```

>This command has an extra backslash `\` after the `\n` and lacks another observation to add in the data frame, relative to the number of colnames. When executed, it produces a tibble dataframe that has an `NA` for the observation under col `b`.

```{r}
read_csv("a,b\n1,2\na,b")
```

>The "error" per se depends entirely on the goal of the code. In one sense, both cols are treated as characters, since they contain letters and numbers (produces a row of 1,2 and a row of a,b). If the intention was to keep a col of integers and characters, then the syntax needs to change to contain one observation set per row (one integer, one character). Additionally, if there was any intention to include `NA`s, they were not marked clearly either.

```{r}
read_csv("a;b\n1;3")
```

>This code uses semicolons to separate data, which is supported by `read_csv2()` not `read_csv()`. The code produces a single element with the name "a;b" and the observation "1;3" as a character.

###Section 11.3
####Parsing a vector

`parse_*()` functions turn vectors into more specific classes.

```{r}
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))
```

These functions all follow the same form: start with a character vector, then specify what `NA` values should be.

```{R}
parse_integer(c("1", "231", ".", "456"), na = ".")
```

If parsing fails, you get warnings:
```{r}
x <- parse_integer(c("123", "345", "abc", "123.45"))
```

You will still get your object, but there will also be a "problems" attribute and the problem data will be converted to `NA`. If the problems is too long, you can use `problems()` to get the full list as a tibble.

Here are the __eight most important `parse_*()` functions:__

  1. `parse_logical()`: T/F factors
  2. `parse_integer()`: whole number integers
  3. `parse_double()`: strict numeric parameter
  4. `parse_number()`: more flexibly numeric parameter
  5. `parse_character()`: characters! beware of character encodings
  6. `parse_datetime()`: date and time
  7. `parse_data()`: just date
  8. `parse_time()`: just time
  
Notes on specific parse functions:

A. Numeric

  1. Take care of use of number grouping, such as \, or \. in different parts of the world.
  2. Beware of characters around numbers like \% and \$
  
A way to deal with this is to set the locale with `locale()`

```{r}
parse_double("1.23")
parse_double("1,23", locale = locale(decimal_mark = ","))
```

You can guess your OS' measurement system, but `parse_number()` is flexible enough to handle these problems:

```{r}
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45")
```

Comma grouping can be dealt with via both functions
```{r}
#Used in America
parse_number("$123,456,789")

# Used in many parts of Europe
parse_number("123.456.789", locale = locale(grouping_mark = "."))

# Used in Switzerland
parse_number("123'456'789", locale = locale(grouping_mark = "'"))
```

B. Strings

How are strings understood in a computer? Use `charToRaw()`

```{r my name}
charToRaw("Huron")
```

So each character is represented by a hexadecimal, such that each is a single byte. The code is in ASCII, which works well for English! However, or tricky characters, it helps to know the encoding (default for `readr` is UTF-8):

```{r}
x1 <- "El Ni\xf1o was particularly bad this year"
x2 <- "\x82\xb1\x82\xf1\x82\xc9\x82\xbf\x82\xcd"
parse_character(x1, locale = locale(encoding = "Latin1"))
parse_character(x2, locale = locale(encoding = "Shift-JIS"))
```

If you cannot find the encoding or do not know it, you can use `guess_encoding()`
```{r}
guess_encoding(charToRaw(x1))
guess_encoding(charToRaw(x2))
```

c. Factors

Warnings will occur if an observation isnt in the levels set in the parse:
```{r}
fruit <- c("apple", "banana")
parse_factor(c("apple", "banana", "bananana"), levels = fruit)
```

D. Date-Times, Dates, and Times

`parse_datetime()` expects ISO8601 date-time (organized from largest to smallest): Year, Month, Day, Hour, Minute, Second

>NOTE: If no time is added, midnight is assumed!

`parse_date()` expects four digit year, a - or /, a month (two digits), another - or /, and then the day (two digits)

`parse_time()` expects the hour (two digits), then :, then minutes, optional :, optional seconds, and lastly an optional am/pm

>R does not have a good date/time built in, so use the `hms` package!

```{r install + load hms}
install.packages("hms", dependencies = T)
library(hms)
```

###Exercises 11.3.5
  1. What are the most important arguments to locale()?
  
The most important arguments are: 
  - date_names
  - decimal_mark
  - tz
  - encoding

  2. What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”?
  
```{r}
#locale(decimal_mark = ".", grouping_mark = ".")
locale(decimal_mark = ",")
locale(grouping_mark = ".")
```
You get an error that says they must be different! When you set one to a particular character, the other is defaulted to the other!
  
  3. I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful.
  
They set what the default format is for time and date. Example with German that shows that you can convert longform to the standard form:
```{r datetime formats}
parse_date("10 februar 1991", "%d %B %Y", locale = locale("de"))
```
  
  4. If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly.
  
Thankfully, I do live in the U.S.
  
  5. What’s the difference between read_csv() and read_csv2()?
  
The former reads comma separated files and the latter reads semicolon separated files.

  6. What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out.
  
It looks like Europe uses ISO 8859 encoding, and it seems like Guobiao (GB) is used in most Asian countries. Notably, UTF-8 is used pretty universally.
  
  7. Generate the correct format string to parse each of the following dates and times:
```{r prompt}
d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
t1 <- "1705"
t2 <- "11:15:10.12 PM"
```

```{r parse}
parse_date(d1, format = "%B %d, %Y")
parse_date(d2, format = "%Y-%b-%d")
parse_date(d3, format = "%d-%b-%Y")
parse_date(d4, format = "%B %d (%Y)")
parse_date(d5, format = "%m/%d/%y")
parse_time(t1, format = "%H%M")
parse_time(t2, format = "%I:%M:%OS %p")
```

###Section 11.4
####Parsing a file

`readr` uses the first 1000 rows to guess the class type of each col. Using `guess_parser()`, you can mimic this:
```{r guess_parser}
guess_parser("2010-10-01")
guess_parser("15:01")
guess_parser(c("TRUE", "FALSE"))
guess_parser(c("1", "5", "9"))
guess_parser(c("12,352,561"))
str(parse_guess("2010-10-10"))
```

>When nothing else fits, the heuristic defaults to a character string!

NA's might screw this up if they are present in high numbers in the first 1000 rows. Special cases in the first 1000 rows might do the same.

```{r failed parse example}
challenge <- read_csv(readr_example("challenge.csv"))
problems(challenge)
```

To fix this, we can go col by col to get at the problem, changing the col class type as required (changed `cols()` for `x`:

```{r fix col}
challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
head(challenge)
tail(challenge)
```

>It is worth reading all cols as characters sometimes to avoid problems... you can always use `type_convert()` to see what would work

###Section 11.5
####Writing objects

I am going to be lazy here, since this is something I am already familar with!

`write_tsv()`
`write_csv()`
`write_excel_csv()` works for excel!!!
`write_rds()` and `read_rds()` work for saving the r data in the RDS binary format
the `feather` package seems to work similarly to RDS, but faster?

-----

##Chapter 12

###Section 12.1
####Intro to Tidying Data

**Tidying** data is formatting it in a consistent way. Hilarious parallel quote by the way, Hadley Whickham. *Slow claps* With **tidy** data, you do less *data munging*, or taking data from one form to another.

###Section 12.2
####What is tidy data?

The *three* rules for a tidy dataset are:

  1. Each variable must have its own column.
  2. Each observation must have its own row.
  3. Each value must have its own cell.

###Exercises 12.2.1
  1. Using prose, describe how the variables and observations are organised in each of the sample tables.
    
    - `table1`: is organized in a tidy tibble! Each column contains variables (country, year, cases, population), and each row is a single set of unique values for those four variables with no separation of a single observation across multiple rows.
    - `table2`: is separated into a similar form as the prior table, but cases and population are separated by row, inherently splitting the prior row convention. Thus, type dictates cases or population and count is the number for the type. Each year and country pairing are on two rows.
    - `table3`: also follows the first table convention but combines cases and population into a single variable, rate (the former divided by the latter).
    - `table4a`: separates the data into one table of just cases. The first col is countries and the next two are specific years, 1999 and 2000. Case values that fit that country and year are entered as elements for corresponding spot.
    - `table4b`: is the same as the prior table but for populations.
  
  2. Compute the rate for table2, and table4a + table4b. You will need to perform four operations:
  
    1. Extract the number of TB cases per country per year.
    2. Extract the matching population per country per year.
    3. Divide cases by population, and multiply by 10000.
    4. Store back in the appropriate place.

Which representation is easiest to work with? Which is hardest? Why?

table2:
```{r get rates table2}
tbcases <- tidyr::table2 %>%
  filter(type == "cases")
pops <- tidyr::table2 %>%
  filter(type == "population")
(tbrate <- tbcases %>%
  mutate(rate = ((.$count/pops$count)*1e4)) %>%
  select(country, year, rate) %>%
  bind_cols(., tbcases, pops) %>%
  select(country, year, cases = count, population = count1, rate))
```

table4a and b:
```{r get rates table4a and b}
tbcases <- table4a %>%
  select(`1999`, `2000`) %>%
  unlist(.) %>%
  unname(.)
pops <- table4b %>%
  select(`1999`, `2000`) %>%
  unlist(.) %>%
  unname(.)
rate <- ((tbcases/pops)*1e4)
tbrate <- as_tibble(cbind(country = table4a$country, year = colnames(table4a)[-1], cases = tbcases, population = pops, rate= rate))
```

Quite frankly, they are both awful to work with. The table4 grouping is awful, as it requires some serious datatype and function mixing (I used `as_tibble()` with `cbind()`).

  3. Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first?
  
You have to isolate cases or reconvert table2 to be like table1

```{r}
tidyr::table2 %>%
  filter(type == "cases") %>%
ggplot(., aes(year, count)) + 
  geom_line(aes(group = country), colour = "grey50") + 
  geom_point(aes(colour = country))
```

###Section 12.3
####Spreading and Gathering

`gather()` will help us collect multiple cols into new variables:
```{r gather}
tidyr::table4a %>% 
  gather(`1999`, `2000`, key = "year", value = "cases")
```

With `gather()`, keep the following in mind:

  - data parameters are the cols with the combined data
  - key parameter is the name for the combined col of the data parameters
  - value parameter is the name for the values of the elements the cols to be combined contain

We can do the same for the other `table4b` and then join the results with `left_join()`:
```{r}
tidy4a <- table4a %>% 
  gather(`1999`, `2000`, key = "year", value = "cases")
tidy4b <- table4b %>% 
  gather(`1999`, `2000`, key = "year", value = "population")
left_join(tidy4a, tidy4b)
```

`spread()` does the opposite of `gather()`:
```{r spread}
tidyr::spread(tidyr::table2, key = type, value = count)
```

Like `gather()`, `spread()` uses the same parameters, they just mean something else slightly:

  - the key variable is the col name that classifies the new cols (gives the names)
  - the value variable is the set of elements to be placed in the new cols

###Exercises 12.3.3
  1. Why are gather() and spread() not perfectly symmetrical?
  
```{r}
stocks <- tibble(
  year   = c(2015, 2015, 2016, 2016),
  half  = c(   1,    2,     1,    2),
  return = c(1.88, 0.59, 0.92, 0.17)
)
stocks %>% 
  spread(year, return) %>% 
  gather("year", "return", `2015`:`2016`)
```

It looks like spread in this case moves to strictly numeric data but then in the gather step, the years are converted to character because they were previously colnames!

  Both spread() and gather() have a convert argument. What does it do?

The `gather()` version will automatically try to correct the error above! The `spread()` version is the source of this error that prevents loss of data?

  2. Why does this code fail?
```{r code fail}
table4a %>% 
  gather(1999, 2000, key = "year", value = "cases")
```
The nonsyntactic characters are not dealt with correctly! They need tick marks.

  3. Why does spreading this tibble fail? How could you add a new column to fix the problem?
```{r tibble fail}
people <- tribble(
  ~name,             ~key,    ~value,
  #-----------------|--------|------
  "Phillip Woods",   "age",       45,
  "Phillip Woods",   "height",   186,
  "Phillip Woods",   "age",       50,
  "Jessica Cordero", "age",       37,
  "Jessica Cordero", "height",   156
)

spread(people, key = key, value = value)
```

The tibble spread fails, because there is an individual with two age measurements. We can add a temporal series within individual column so that the rows become unique.

  4. Tidy the simple tibble below. Do you need to spread or gather it? What are the variables?
```{r tidy some data}
preg <- tribble(
  ~pregnant, ~male, ~female,
  "yes",     NA,    10,
  "no",      20,    12
)

preg %>%
  gather(c(male, female), key = "sex", value = "count")
```

This data was hard to interpret initially, but then I realized it was a table of pregnancy by sex for a surveyed group. Since we specify the data, we can just give a name for the `value` parameter of something like n or counts.

###Section 12.4
####Separating and Uniting

`separate()` will take values from elements in an object and pull them apart. The default is to find the non-alphanumeric chracters in the element and split them accordingly.

```{r separate example}
table3 %>% 
  separate(rate, into = c("cases", "population"))
```

That being said, you can split by specific characters, using regular expressions or by position (if numeric values is used). Positive values will find position from the left, and negative values will do the same from the right. Additionally, the `convert = TRUE` argument will convert the col types for the newly created cols.
```{r separate example with / and numeric}
table3 %>% 
  separate(rate, into = c("cases", "population"), sep = "/", convert = TRUE)
#numeric example
table3 %>% 
  separate(year, into = c("century", "year"), sep = 2)
```

`unite()` is the converse of `separate()`, and thus combines values from multiple cols into a sinle new one.
```{r unite example}
tidyr::table5 %>% 
  unite(new, century, year)
```

There is also a `sep =` argument for `unite()`, which by default is an underscore `_`.
```{r unite example v2}
tidyr::table5 %>% 
  unite(new, century, year, sep = "")
```

###Exercises 12.4.3
  1. What do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets.
```{r}
tibble(x = c("a,b,c", "d,e,f,g", "h,i,j")) %>% 
  separate(x, c("one", "two", "three"))

tibble(x = c("a,b,c", "d,e", "f,g,i")) %>% 
  separate(x, c("one", "two", "three"))
```

`extra=` will handle extra characters accordingly. You can set to `"warn"`, `"drop"`, and `"merge"`. `fill=` deals with not enough elements, and has the following options: `"warn"`, `"right"`, and `"left"`. The left and right will fill missing values on that side. *Note that the `data=` must be a dataframe!*
```{r show em examples}
dummy <- tibble(x = c("a,b,c", "d,e,f,g", "h,i,j"))

dummy %>%
  separate(x, c("one", "two", "three"), extra = "warn")
dummy %>%
  separate(x, c("one", "two", "three"), extra = "drop")
dummy %>%
  separate(x, c("one", "two", "three"), extra = "merge")

```

>So we see that `"warn"` and `"drop"` do the same thing functionally, but the former adds the warning message.

```{r show em examples for fill}
dummy <- tibble(x = c("a,b,c", "d,e", "f,g,i"))

dummy %>% 
  separate(x, c("one", "two", "three"), fill = "warn")
dummy %>% 
  separate(x, c("one", "two", "three"), fill = "left")
dummy %>% 
  separate(x, c("one", "two", "three"), fill = "right")
```

>Now in this case, we can see that `"warn"` and `"right"` behave similarly in function. Notably, the `NA` is in the first col for `"left"`.

  2. Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?
  
In both cases, `remove = TRUE` is the default parameter setting and removes  the input column from the output data frame. This is helpful, because typically, you do not want the original column, as it contains aberrant versions of data. However, if you want to check that the function performed as expected, you might set it to `FALSE` to be sure before re-running with `TRUE`.

  3. Compare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite?
  
So `extract()` will group new cols by a regular expression according to the number of new cols. `separate()` will only use a single regex to separate by. Notably, there are three different ways to separate data, but only one for unite, as there are many ways to partition a col, but really only one way to combine multiples into a single col.

###Section 12.5
####Missing Values

Data can be treated as missing in two ways:
  1. **Explicit**: specifically treated as `NA`-- "presence of an absence"
  2. **Implicit**: just missing from the dataset-- "absence of a presence"
  
We can use data spreading with a default to fill implied missing values with `NA` with `fill = NA` (with `spread()`). Conversely, when trying to get data read in or combined, you can use `na.rm = TRUE` to drop explicit missing data (with `gather()`).

Notably, `complete()` can help with missing values too to get implicit ones represented explicitly. This involves getting all unique combinations of a set of cols and makes sure that the dataset contains all of those values, filling in with `NA`s as needed:
```{r complete example}
stocks <- tibble(
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
)

stocks %>%
  complete(year, qtr)
```

Normally when reading in data, missing data help hold the shape of the dataframe, but one can use `fill()` to convert `NA`s in a col to actual values:

```{r}
(treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
))

treatment %>% 
  fill(person)
```

###Exercises 12.5.1
  1. Compare and contrast the fill arguments to spread() and complete().

For `spread()`, `fill =` will populate both explicit and implicit missing data with the corresponding parameter value (default is `NA`). Conversely, `fill =` for `complete()` requires a list that will be used to replace missing values with instead of `NA`s specifically. The latter allows for more flexibility in replacing missing data.
  
  2. What does the direction argument to fill() do?
  
`direction =` is set to either `"up"` or `"down"`, which dictates the direction that filling `NA` values occurs. Default is `"down"`.

###Section 12.6
####Case Study

```{r read in who tb data}
tidyr::who
```

So it looks like `country`, `iso2`, and `iso3` all refer to the same thing. `year` is also an obvious variable.

The rest of the cols are uncertain, we should gather them to figure out more and call `key` and get counts with `value`:
```{r gather other cols}
(who1 <- tidyr::who %>% 
  gather(new_sp_m014:newrel_f65, key = "key", value = "cases", na.rm = TRUE))
```

Let's use `count()` to get more info:
```{r count key}
who1 %>%
  count(key)
```

Using the data dictionary, we can better understand each value for key. Note that there is a transcription/data consistency error with `newrel`, which should be `new_rel`. Let's fix that with `str_replace()`:
```{r fix newrel}
(who2 <- who1 %>% 
  mutate(key = stringr::str_replace(key, "newrel", "new_rel")))
```

Now, we can use `separate()` to split codes by `_` to start. We can also drop the `new`, `iso2`, and `iso3` cols, since they are redundant or not informative.

```{r separate some stuff}
(who3 <- who2 %>% 
  separate(key, c("new", "type", "sexage"), sep = "_"))
(who4 <- who3 %>%
  select(-new, -iso2, -iso3))
```

We then need to break up age class and sex by chopping the first character from the rest:
```{r break sexage}
(who5 <- who4 %>%
   separate(sexage, into = c("sex", "age"), sep = 1)
   )
```

Now we have a tidy dataset! Here is what it looks like in one big piping step:
```{r tidy who}
tidyr::who %>%
  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  mutate(code = stringr::str_replace(code, "newrel", "new_rel")) %>%
  separate(code, c("new", "var", "sexage")) %>% 
  select(-new, -iso2, -iso3) %>% 
  separate(sexage, c("sex", "age"), sep = 1)
```

###Exercises 12.6.1
  1. In this case study I set na.rm = TRUE just to make it easier to check that we had the correct values. Is this reasonable? Think about how missing values are represented in this dataset. Are there implicit missing values? What’s the difference between an NA and zero?
  
This is probably not the best idea, as there are likely a few implicitly missing values, since there is no promise that all years had cases for all countries. `NA` implies no record exists, whereas `0` suggests no cases (big difference!). Let's try to show that there are implied missing data:
```{r find implicit missing datat}
tidyr::who  %>%
  #gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  group_by(country) %>%
  summarise(count = n()) %>%
ggplot(.) +
  geom_bar(mapping = aes(x = country, y = count), stat = "identity") +
  coord_flip()           

#also you can find the number of countries, then group by the countries and years, then see if that grouping is divisible by total number of countries.

length(unique(tidyr::who$country))  #219

tidyr::who  %>%
  group_by(country, year) %>%
  summarise(count = n()) %>%
  nrow(.)/219
  
```

So we can see not every country is represented in the rows equally, which is a sign of implicit missing data (we already know explicit missing data due to the `NA`s we see when looking at the dataset as is). Secondarily, we can confirm this with the second block of code. If every country only had explicit missing data, the number of rows as summarized with `group_by(country, year)` would be divisible by the total number of countries, which is `219`. That is not the case, since we do not get an integer for the quotient.

  2. What happens if you neglect the mutate() step? (mutate(key = stringr::str_replace(key, "newrel", "new_rel")))
  
```{r tidy who - mutate}
tidyr::who %>%
  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  #mutate(code = stringr::str_replace(code, "newrel", "new_rel")) %>%
  separate(code, c("new", "var", "sexage")) %>% 
  select(-new, -iso2, -iso3) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>%
  arrange(var)
```  

Now there are a bunch of issues with the var names, because there is one less signal to parse by (missing `_`). Therefore, some values are coerced to be NA. This really messes up the dataset. You can see this by looking for `var = "rel"` in the original version versus above. 
  
  3. I claimed that iso2 and iso3 were redundant with country. Confirm this claim.
  
```{r confirm redundancy}  
tidyr::who %>%
  group_by(country, iso2, iso3) %>%
  summarise(n()) %>%
  nrow(.)

#vs
tidyr::who %>%
  select(country) %>%
  distinct(.) %>%
  nrow(.)
```
We can see that even if we group by all three variables, we end up with the same number of rows. The first code determines groupings of all three, which would have multiple instances for a country with unique `iso` values if they did not map perfectly. Instead, we end up with a value of 219, which corresponds to the number of unique country values in the second bit of code.

  4. For each country, year, and sex compute the total number of cases of TB. Make an informative visualisation of the data.
  
```{r get and comapare tb data}
(whoo <- tidyr::who %>%
  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  mutate(code = stringr::str_replace(code, "newrel", "new_rel")) %>%
  separate(code, c("new", "var", "sexage")) %>% 
  select(-new, -iso2, -iso3) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>%
   group_by(country, year, sex) %>%
   summarise(counts = sum(value)))

#now visualize
ggplot(data = whoo) +
  geom_line(mapping = aes(x = year, y = counts, color = country), show.legend = F) +
  facet_grid(.~sex) +
  ylim(0, 8e5)

```